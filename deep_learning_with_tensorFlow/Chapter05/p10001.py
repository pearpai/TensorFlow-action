import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# MNIST æ•°æ®é›†çš„ç›¸å…³å¸¸æ•°
# è¾“å…¥å±‚çš„èŠ‚ç‚¹æ•°ã€‚å¯¹äºMNISTæ•°æ®é›†ï¼Œè¿™ä¸ªå°±ç­‰äºå›¾ç‰‡çš„åƒç´ 
INPUT_NODE = 784
# è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°ã€‚è¿™ä¸ªç­‰äºç±»åˆ«çš„æ•°ç›®ã€‚å› ä¸ºåœ¨MNISTæ•°æ®é›†ä¸­
# éœ€è¦åŒºåˆ†çš„æ˜¯0~9è¿™10ä¸ªæ•°å­—ï¼Œæ‰€ä»¥è¿™é‡Œè¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°ä¸º10
OUTPUT_NODE = 10

# é…ç½®ç¥ç»ç½‘ç»œå‚æ•°
# éšè—å±‚èŠ‚ç‚¹æ•°ã€‚è¿™é‡Œä½¿ç”¨åªæœ‰ä¸€ä¸ªéšè—å±‚çš„ç½‘ç»œç»“æ„ä½œä¸ºæ ·ä¾‹
# è¿™ä¸ªéšè—å±‚æœ‰500ä¸ªèŠ‚ç‚¹
LAYER1_NODE = 500

# ä¸€ä¸ªè®­ç»ƒbatchä¸­çš„è®­ç»ƒæ•°æ®ä¸ªæ•°ã€‚æ•°å­—è¶Šå°æ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹çº¦æ¥è¿‘éšæœºæ¢¯åº¦ä¸‹é™ï¼›
# æ•°å­—è¶Šå¤§æ—¶ï¼Œè®­ç»ƒè¶Šæ¥è¿‘æ¢¯åº¦ä¸‹é™
BATCH_SIZE = 100

# åŸºç¡€å­¦ä¹ ç‡
LEARNING_RATE_BASE = 0.8
# å­¦ä¹ ç‡çš„è¡°å‡ç‡
LEARNING_RATE_DECAY = 0.99
# æè¿°æ¨¡å‹å¤æ‚åº¦çš„æ­£åˆ™åŒ–é¡¹åœ¨æŸå¤±å‡½æ•°ä¸­çš„ç³»æ•°
REGULARAZTION_RATE = 0.0001
# è®­ç»ƒè½®æ•°
TRAINING_STEPS = 30000
# æ»‘åŠ¨å¹³å‡è¡°å‡ç‡
MOVING_AVERAGE_DECAY = 0.99


# ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç»™å®šç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œæ‰€æœ‰å‚æ•°ï¼Œè®¡ç®—ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­ç»“æœã€‚åœ¨è¿™é‡Œ
# å®šä¹‰äº†ä¸€ä¸ªä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„ä¸‰å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œé€šè¿‡åŠ å…¥éšè—å±‚æ—¶é—´äº†å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„
# é€šè¿‡ReLUæ¿€æ´»å‡½æ•°å®ç°äº†å»çº¿æ€§åŒ–ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ä¹Ÿæ”¯æŒä¼ å…¥ç”¨äºè®¡ç®—å‚æ•°å¹³å‡å€¼çš„ç±»
# è¿™æ ·æ–¹ä¾¿åœ¨æµ‹è¯•æ—¶ä½¿ç”¨æ»‘åŠ¨å¹³å‡æ¨¡å‹
def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):
    # å½“æ²¡æœ‰æä¾›æ¬¢åŠ¨å¹³å‡ç±»æ—¶ï¼Œç›´æ¥ä½¿ç”¨å‚æ•°å½“å‰çš„å–å€¼
    if avg_class is None:
        # è®¡ç®—éšè—å±‚çš„å‰å‘ä¼ æ’­ç»“æœï¼Œè¿™é‡Œä½¿ç”¨äº†ReLUæ¿€æ´»å‡½æ•°
        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)
        # è®¡ç®—è¾“å‡ºå±‚çš„å‰å‘ä¼ æ’­ç»“æœã€‚å› ä¸ºåœ¨è®¡ç®—æŸå¤±å‡½æ•°æ—¶ä¼šä¸€å¹¶è®¡ç®— sofmaxå‡½æ•°ï¼Œ
        # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦åŠ å…¥æ¿€æ´»å‡½æ•°ã€‚è€Œä¸”ä¸åŠ å…¥softmaxä¸ä¼šå½±å“é¢„æµ‹ç»“æœã€‚å› ä¸ºé¢„æµ‹æ—¶
        # ä½¿ç”¨çš„æ˜¯ä¸åŒç±»åˆ«çš„èŠ‚ç‚¹è¾“å‡ºå€¼çš„ç›¸å¯¹å¤§å°ï¼Œæœ‰æ²¡æœ‰softmaxå±‚å¯¹æœ€åçš„åˆ†ç±»ç»“æœçš„
        # è®¡ç®—æ²¡æœ‰å½±å“ã€‚é¢„ç®—åœ¨è®¡ç®—æ•´ä¸ªç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­æ—¶å¯ä»¥ä¸åŠ å…¥æœ€åçš„softmaxå±‚ã€‚
        return tf.matmul(layer1, weights2) + biases2
    else:
        # é¦–å…ˆä½¿ç”¨avg_class.averageå‡½æ•°æ¥è®¡ç®—å¾—å‡ºå˜é‡çš„æ»‘åŠ¨å¹³å‡å€¼
        # ç„¶åå†è®¡ç®—ç›¸åº”çš„ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­ç»“æœ
        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))
        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)


def train(mnist):
    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')
    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')
    # ç”Ÿæˆéšè—å±‚å‚æ•°
    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))
    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))

    # ç”Ÿæˆè¾“å‡ºå±‚å‚æ•°
    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))
    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))

    # è®¡ç®—ä¸å«æ»‘åŠ¨å¹³å‡ç±»çš„å‰å‘ä¼ æ’­ç»“æœã€‚è¿™é‡Œç»™å‡ºçš„ç”¨äºè®¡ç®—æ»‘åŠ¨å¹³å‡çš„ç±»ä¸ºNoneï¼Œ
    # æ‰€ä»¥å‡½æ•°ä¸ä¼šä½¿ç”¨å‚æ•°çš„æ»‘åŠ¨å¹³å‡å€¼
    y = inference(x, None, weights1, biases1, weights2, biases2)

    # å®šä¹‰è®­ç»ƒè½®æ•°åŠç›¸å…³æ»‘åŠ¨å¹³å‡ç±»
    # å®šä¹‰å­˜å‚¨è®­ç»ƒè½®æ•°çš„å˜é‡ã€‚è¿™ä¸ªå˜é‡ä¸éœ€è¦è®¡ç®—æ»‘åŠ¨å¹³å‡å€¼ï¼Œæ‰€ä»¥è¿™é‡Œåˆ¶å®šè¿™ä¸ªå˜é‡ä¸º
    # ä¸å¯è®­ç»ƒçš„å˜é‡ï¼ˆtrainable=Falseï¼‰ã€‚åœ¨ä½¿ç”¨TensorFlowè®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œ
    # ä¸€èˆ¬ä¼šå°†ä»£è¡¨è®­ç»ƒè½®æ•°çš„å˜é‡åˆ¶å®šä¸ºä¸å¯è®­ç»ƒçš„å‚æ•°
    global_step = tf.Variable(0, trainable=False)
    # ç»™å®šæ»‘åŠ¨å¹³å‡è¡°å‡ç‡å’Œè®­ç»ƒè½®æ•°çš„å˜é‡ï¼Œåˆå§‹åŒ–æ»‘åŠ¨å¹³å‡ç±»ã€‚åœ¨ç¬¬4ç« ä¸­ä»‹ç»è¿‡ç»™
    # å®šè®­ç»ƒè½®æ•°çš„å˜é‡å¯ä»¥åŠ å¿«è®­ç»ƒæ—©èµ·å˜é‡çš„æ›´æ–°é€Ÿåº¦
    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)

    # åœ¨æ‰€æœ‰ä»£è¡¨ç¥ç»ç½‘ç»œå‚æ•°çš„å˜é‡ä¸Šä½¿ç”¨æ»‘åŠ¨å¹³å‡ã€‚å…¶ä»–è¾…åŠ©å˜é‡ï¼ˆæ¯”å¦‚global_stepï¼‰å°±
    # ä¸éœ€è¦äº†ã€‚tf.trainable_variablesè¿”å›çš„å°±æ˜¯å›¾ä¸Šçš„é›†åˆ
    # GraphKeys.TRAINABLE_VARIABLESä¸­çš„å…ƒç´ ã€‚è¿™ä¸ªé›†åˆçš„å…ƒç´ å°±æ˜¯æ‰€æœ‰æ²¡æœ‰æŒ‡å®š
    # trainable=False çš„å‚æ•°
    variable_averages_op = variable_averages.apply(tf.trainable_variables())

    # è®¡ç®—ä½¿ç”¨äº†æ»‘åŠ¨å¹³å‡ä¹‹åçš„å‘å‰ä¼ æ’­ç»“æœã€‚ç¬¬4ç« ä¸­ä»‹ç»è¿‡æ»‘åŠ¨å¹³å‡ä¸ä¼šæ”¹å˜æœ¬äº‹çš„å–å€¼
    # è€Œæ˜¯ç»´æŠ¤äº†ä¸€ä¸ªå½±å­å˜é‡æ¥è®°å½•æ»‘åŠ¨å¹³å‡å€¼ã€‚æ‰€ä»¥å½“éœ€è¦ä½¿ç”¨è¿™ä¸ªæ»‘åŠ¨å¹³å‡å€¼æ—¶ï¼Œéœ€è¦æ˜ç¡®è°ƒç”¨averageå‡½æ•°
    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)

    # è®¡ç®—äº¤å‰ç†µæœºå™¨å¹³å‡å€¼
    # è®¡ç®—äº¤å‰ç†µä½œä¸ºåˆ»ç”»é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´å·®è·çš„æŸå¤±å‡½æ•°ã€‚è¿™é‡Œä½¿ç”¨äº†TensorFlowä¸­æä¾›çš„
    # sparse_softmax_cross_entropy_with_logitså‡½æ•°æ¥è®¡ç®—äº¤å‰ç†µã€‚å½“åˆ†ç±»
    # é—®é¢˜åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥åŠ é€Ÿäº¤å‰ç†µçš„è®¡ç®—ã€‚MNISTé—®é¢˜çš„å›¾ç‰‡ä¸­
    # å‚æ•°æ˜¯ç¥ç»ç½‘ç»œä¸åŒ…æ‹¬softmaxå±‚çš„å‰å‘ä¼ æ’­ç»“æœï¼Œç¬¬äºŒä¸ªæ˜¯è®­ç»ƒæ•°æ®çš„æ­£ç¡®ç­”æ¡ˆã€‚å› ä¸ºæ ‡å‡†å¤§éš¾æ˜¯é•¿åº¦ä¸º10çš„ä¸€ç»´æ•°ç»„
    # è€Œè¯¥å‡½æ•°éœ€è¦æä¾›çš„æ˜¯æ­£ç¡®ç­”æ¡ˆğŸ”¢ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨tf.arg_maxæ¥å¾—åˆ°æ­£ç¡®ç­”æ¡ˆå¯¹äºçš„ç±»åˆ«ç¼–å·
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.arg_max(y_, 1))
    # è®¡ç®—åœ¨å½“å‰batchä¸­æ‰€æœ‰æ ·ä¾‹çš„äº¤å‰ç†µå¹³å‡å€¼
    cross_entropy_mean = tf.reduce_mean(cross_entropy)

    # è®¡ç®—L2æ­£åˆ™åŒ–æŸå¤±å‡½æ•°
    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)
    # è®¡ç®—æ¨¡å‹çš„æ­£åˆ™åŒ–æŸå¤±ã€‚ä¸€èˆ¬å­è®¡ç®—ç¥ç»ç½‘ç»œè¾¹ä¸Šæƒé‡çš„æ­£åˆ™åŒ–æŸå¤±ï¼Œè€Œä¸ä½¿ç”¨åç½®é¡¹ã€‚
    regularaztion = regularizer(weights1) + regularizer(weights2)
    # æ€»æŸå¤±ç­‰äºäº¤å‰ç†µæŸå¤±å’Œæ­£æŠ“æŸå¤±çš„å’Œ
    loss = cross_entropy_mean + regularaztion

    # è®¾ç½®æŒ‡æ•°è¡°å‡çš„å­¦ä¹ ç‡
    learning_rate = tf.train.exponential_decay(
        LEARNING_RATE_BASE,  # åŸºç¡€çš„å­¦ä¹ ç‡ï¼Œéšç€è¿­ä»£çš„è¿›è¡Œï¼Œæ›´æ–°å˜é‡æ—¶ä½¿ç”¨å­¦ä¹ ç‡åœ¨è¿™ä¸ªåŸºç¡€ä¸Šé€’å‡
        global_step,  # å½“å‰è¿­ä»£çš„è½®æ•°
        mnist.train.num_examples / BATCH_SIZE,  # è¿‡å®Œæ‰€æœ‰çš„è®­ç»ƒæ•°æ®éœ€è¦çš„è¿­ä»£æ¬¡æ•°
        LEARNING_RATE_DECAY,  # å­¦ä¹ ç‡è¡°å‡é€Ÿåº¦
        staircase=True
    )

    # ä¼˜åŒ–æŸå¤±å‡½æ•°
    # ä½¿ç”¨ tf.train.GradientDescentOptimizer ä¼˜åŒ–ç®—æ³•æ¥ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚æ³¨æ„è¿™é‡ŒæŸå¤±å‡½æ•°
    # åŒ…å«äº†äº¤å‰ç†µæŸå¤±å’ŒL2æ­£åˆ™åŒ–æŸå¤±
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

    #
    # åå‘ä¼ æ’­æ›´æ–°å‚æ•°å’Œæ›´æ–°æ¯ä¸€ä¸ªå‚æ•°çš„æ»‘åŠ¨å¹³å‡å€¼
    with tf.control_dependencies([train_step, variable_averages_op]):
        train_op = tf.no_op(name='train')

    # è®¡ç®—æ­£ç¡®ç‡
    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    # åˆå§‹åŒ–ä¼šè¯å¹¶å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    with tf.Session() as sess:
        tf.global_variables_initializer().run()
        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}
        test_feed = {x: mnist.test.images, y_: mnist.test.labels}
        for i in range(TRAINING_STEPS):
            if i % 1000 == 0:
                validate_acc = sess.run(accuracy, feed_dict=validate_feed)
                test_acc = sess.run(accuracy, feed_dict=test_feed)
                print("After %d training step(s), validation accuracy using average model is %g,"
                      "test accuracy using average model is %g " % (i, validate_acc, test_acc))

            xs, ys = mnist.train.next_batch(BATCH_SIZE)
            sess.run(train_op, feed_dict={x: xs, y_: ys})

        # è®¡ç®—æ»‘åŠ¨å¹³å‡æ¨¡å‹å’‹æµ‹è¯•æ•°æ®å’ŒéªŒè¯æ•°æ®ä¸Šçš„æ­£ç¡®ç‡
        test_acc = sess.run(accuracy, feed_dict=test_feed)
        print(("After %d training step(s), test accuracy using average model is %g" % (TRAINING_STEPS, test_acc)))


def main(argv=None):
    mnist = input_data.read_data_sets("../datasets/MNIST_data", one_hot=True)
    train(mnist)


if __name__ == '__main__':
    main()
